{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M9LqnyjU1tD"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse the quoted string of values into a list of floats\n",
        "def parse_velocity_string(velocity_string):\n",
        "    return list(map(float, velocity_string.strip('\"').split(',')))"
      ],
      "metadata": {
        "id": "Pgs6JwWYU-0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and process velocity data\n",
        "def load_velocity_data(x_file, z_file, u_file, w_file):\n",
        "    x_coords = pd.read_csv(x_file).values.flatten()\n",
        "    z_coords = pd.read_csv(z_file).values.flatten()\n",
        "\n",
        "    u_velocities_df = pd.read_csv(u_file, header = None)\n",
        "    w_velocities_df = pd.read_csv(w_file, header = None)\n",
        "\n",
        "    u_velocities = u_velocities_df[0].apply(parse_velocity_string).values\n",
        "    w_velocities = w_velocities_df[0].apply(parse_velocity_string).values\n",
        "\n",
        "    u_velocities = np.array(u_velocities.tolist())\n",
        "    w_velocities = np.array(w_velocities.tolist())\n",
        "\n",
        "    expected_z_size = len(z_coords) + 1\n",
        "    expected_x_size = len(x_coords) + 1\n",
        "\n",
        "    if u_velocities.shape != (expected_z_size, expected_x_size):\n",
        "        raise ValueError(f\"u_velocities shape does not match the expected grid shape. Got {u_velocities.shape}, expected {(expected_z_size, expected_x_size)}\")\n",
        "    if w_velocities.shape != (expected_z_size, expected_x_size):\n",
        "        raise ValueError(f\"w_velocities shape does not match the expected grid shape. Got {w_velocities.shape}, expected {(expected_z_size, expected_x_size)}\")\n",
        "\n",
        "    X_grid, Z_grid = np.meshgrid(x_coords, z_coords)\n",
        "    X_flat = X_grid.flatten()\n",
        "    Z_flat = Z_grid.flatten()\n",
        "    X = np.column_stack((X_flat, Z_flat))\n",
        "\n",
        "    u_flat = u_velocities.flatten()\n",
        "    w_flat = w_velocities.flatten()\n",
        "    y = np.column_stack((u_flat, w_flat))\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "d5vO-B5DVCUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process data for Reynolds number 180\n",
        "X_180, y_180 = load_velocity_data('x_768_128.csv', 'z_768_128.csv', 'U_768_128.csv', 'W_768_128.csv')\n",
        "\n",
        "# Load and process data for Reynolds number 400\n",
        "X_400, y_400 = load_velocity_data('x_512_320.csv', 'z_512_320.csv', 'U_512_320.csv', 'W_512_320.csv')\n",
        "\n",
        "# Combine the datasets\n",
        "X_combined = np.vstack((X_180, X_400))\n",
        "y_combined = np.vstack((y_180, y_400))"
      ],
      "metadata": {
        "id": "uXr3S89LVKnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the machine learning model\n",
        "model = Sequential([\n",
        "    Dense(64, activation  = 'relu', input_shape = (2,)),\n",
        "    Dense(64, activation = 'relu'),\n",
        "    Dense(2)\n",
        "])\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(X_combined, y_combined, epochs = 200, batch_size = 32, validation_split = 0.2)"
      ],
      "metadata": {
        "id": "deMmsLH1VKpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict velocities for Reynolds number 1000\n",
        "new_x_coords = pd.read_csv('x1000_768_128.csv').values.flatten()\n",
        "new_z_coords = pd.read_csv('z1000_768_128.csv').values.flatten()\n",
        "\n",
        "new_X_grid, new_Z_grid = np.meshgrid(new_x_coords, new_z_coords)\n",
        "new_X_flat = new_X_grid.flatten()\n",
        "new_Z_flat = new_Z_grid.flatten()\n",
        "new_X = np.column_stack((new_X_flat, new_Z_flat))\n",
        "\n",
        "predicted_velocities = model.predict(new_X)\n",
        "\n",
        "predicted_u_velocities = predicted_velocities[:, 0]\n",
        "predicted_w_velocities = predicted_velocities[:, 1]"
      ],
      "metadata": {
        "id": "30SylbIbVKsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the predicted velocities to CSV files\n",
        "pd.DataFrame(predicted_u_velocities).to_csv('predicted_u_velocities_1000.csv', index = False)\n",
        "pd.DataFrame(predicted_w_velocities).to_csv('predicted_w_velocities_1000.csv', index = False)"
      ],
      "metadata": {
        "id": "4Fck19yEVKuq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}